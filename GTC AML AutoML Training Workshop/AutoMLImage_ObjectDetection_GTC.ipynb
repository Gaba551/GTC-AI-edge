{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Training an Object Detection model using AutoML\n",
    "In this notebook, we go over how you can use AutoML for training an Object Detection model. We will use a small dataset to train the model, demonstrate how you can tune hyperparameters of the model to optimize model performance and deploy the model to use in inference scenarios. For detailed information please refer to the [documentation of AutoML for Images](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](example_object_detection_predictions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** This feature is currently in public preview. This preview version is provided without a service-level agreement. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/en-us/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Please follow the [\"Setup a new conda environment\"](https://github.com/Azure/azureml-examples/tree/main/python-sdk/tutorials/automl-with-azureml#3-setup-a-new-conda-environment) instructions to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: azureml-sdk in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-train-core~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-dataset-runtime[fuse]~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-core~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-pipeline~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-train-automl-client~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-train-restclients-hyperdrive~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-train-core~=1.38.0->azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-telemetry~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-train-core~=1.38.0->azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy!=1.19.3; sys_platform == \"linux\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: azureml-dataprep<2.27.0a,>=2.26.0a in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (2.26.0)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow<4.0.0,>=0.17.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: fusepy<4.0.0,>=3.0.1; extra == \"fuse\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<=1.26.7,>=1.23 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-resource<21.0.0,>=15.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (20.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests[socks]<3.0.0,>=2.19.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (2.26.0)\n",
      "Requirement already satisfied, skipping upgrade: msal<2.0.0,>=1.15.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: ndg-httpsclient<=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: azure-common<2.0.0,>=1.1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (1.1.27)\n",
      "Requirement already satisfied, skipping upgrade: docker<6.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (4.4.4)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: azure-core<1.22 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (1.19.1)\n",
      "Requirement already satisfied, skipping upgrade: pyopenssl<22.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (20.0.1)\n",
      "Requirement already satisfied, skipping upgrade: azure-graphrbac<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.61.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.7.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: contextlib2<22.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.6.0.post1)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-storage<20.0.0,>=16.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (19.0.0)\n",
      "Requirement already satisfied, skipping upgrade: paramiko<3.0.0,>=2.0.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: argcomplete<2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (1.12.3)\n",
      "Requirement already satisfied, skipping upgrade: backports.tempfile in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (1.0)\n",
      "Requirement already satisfied, skipping upgrade: humanfriendly<11.0,>=4.7 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (9.2)\n",
      "Requirement already satisfied, skipping upgrade: pkginfo in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-authorization<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.61.0)\n",
      "Requirement already satisfied, skipping upgrade: msrest<1.0.0,>=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.6.21)\n",
      "Requirement already satisfied, skipping upgrade: jsonpickle<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging<22.0,>=20.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (21.2)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-containerregistry<9.0.0,>=8.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (8.2.0)\n",
      "Requirement already satisfied, skipping upgrade: knack~=0.8.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: adal<=1.2.7,>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (1.2.7)\n",
      "Requirement already satisfied, skipping upgrade: SecretStorage<4.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (3.3.1)\n",
      "Requirement already satisfied, skipping upgrade: pathspec<1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: msal-extensions<0.4,>=0.3.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.3.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (3.4.8)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-keyvault<10.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: msrestazure<=0.6.4,>=0.4.33 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (0.6.4)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.38.0->azureml-sdk) (2021.3)\n",
      "Requirement already satisfied, skipping upgrade: azureml-pipeline-core~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-pipeline~=1.38.0->azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-pipeline-steps~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-pipeline~=1.38.0->azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-automl-core~=1.38.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-train-automl-client~=1.38.0->azureml-sdk) (1.38.0)\n",
      "Requirement already satisfied, skipping upgrade: applicationinsights in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-telemetry~=1.38.0->azureml-train-core~=1.38.0->azureml-sdk) (0.11.10)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle<3.0.0,>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataprep<2.27.0a,>=2.26.0a->azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: dotnetcore2<3.0.0,>=2.1.14 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataprep<2.27.0a,>=2.26.0a->azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (2.1.21)\n",
      "Requirement already satisfied, skipping upgrade: azureml-dataprep-native<39.0.0,>=38.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataprep<2.27.0a,>=2.26.0a->azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (38.0.0)\n",
      "Requirement already satisfied, skipping upgrade: azure-identity==1.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataprep<2.27.0a,>=2.26.0a->azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: azureml-dataprep-rslex~=2.2.0dev0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataprep<2.27.0a,>=2.26.0a->azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-core<2.0.0,>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-mgmt-resource<21.0.0,>=15.0.0->azureml-core~=1.38.0->azureml-sdk) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.38.0->azureml-sdk) (2021.10.8)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.38.0->azureml-sdk) (2.0.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.38.0->azureml-sdk) (3.3)\n",
      "Requirement already satisfied, skipping upgrade: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.38.0->azureml-sdk) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.38.0->azureml-sdk) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.4.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from docker<6.0.0->azureml-core~=1.38.0->azureml-sdk) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from docker<6.0.0->azureml-core~=1.38.0->azureml-sdk) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: bcrypt>=3.1.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml-core~=1.38.0->azureml-sdk) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pynacl>=1.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml-core~=1.38.0->azureml-sdk) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata<5,>=0.23; python_version == \"3.6\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from argcomplete<2.0->azureml-core~=1.38.0->azureml-sdk) (4.8.1)\n",
      "Requirement already satisfied, skipping upgrade: backports.weakref in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from backports.tempfile->azureml-core~=1.38.0->azureml-sdk) (1.0.post1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.38.0->azureml-sdk) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: isodate>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.38.0->azureml-sdk) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing<3,>=2.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from packaging<22.0,>=20.0->azureml-core~=1.38.0->azureml-sdk) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.8.2->azureml-core~=1.38.0->azureml-sdk) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.8.2->azureml-core~=1.38.0->azureml-sdk) (0.8.9)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.8.2->azureml-core~=1.38.0->azureml-sdk) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.8.2->azureml-core~=1.38.0->azureml-sdk) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: jeepney>=0.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from SecretStorage<4.0.0->azureml-core~=1.38.0->azureml-sdk) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msal-extensions<0.4,>=0.3.0->azureml-core~=1.38.0->azureml-sdk) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml-core~=1.38.0->azureml-sdk) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: distro>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from dotnetcore2<3.0.0,>=2.1.14->azureml-dataprep<2.27.0a,>=2.26.0a->azureml-dataset-runtime[fuse]~=1.38.0->azureml-sdk) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata<5,>=0.23; python_version == \"3.6\"->argcomplete<2.0->azureml-core~=1.38.0->azureml-sdk) (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata<5,>=0.23; python_version == \"3.6\"->argcomplete<2.0->azureml-core~=1.38.0->azureml-sdk) (3.10.0.2)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.5.1->azureml-core~=1.38.0->azureml-sdk) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml-core~=1.38.0->azureml-sdk) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade azureml-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was created using version 1.35.0 of the Azure ML SDK.\n",
      "You are currently using version 1.38.0 of the Azure ML SDK.\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"This notebook was created using version 1.35.0 of the Azure ML SDK.\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK.\")\n",
    "assert (\n",
    "    azureml.core.VERSION >= \"1.35\"\n",
    "), \"Please upgrade the Azure ML SDK by running '!pip install --upgrade azureml-sdk' then restart the kernel.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace setup\n",
    "In order to train and deploy models in Azure ML, you will first need to set up a workspace.\n",
    "\n",
    "An [Azure ML Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#workspace) is an Azure resource that organizes and coordinates the actions of many other Azure resources to assist in executing and sharing machine learning workflows. In particular, an Azure ML Workspace coordinates storage, databases, and compute resources providing added functionality for machine learning experimentation, deployment, inference, and the monitoring of deployed models.\n",
    "\n",
    "Create an Azure ML Workspace within your Azure subscription or load an existing workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/83da1f6b-22c0-4300-aa13-bd260eab57e5/resourceGroups/amldatalabeling/providers/Microsoft.MachineLearningServices/workspaces/Computer_Vision_Pipeline',\n",
       " 'name': 'Computer_Vision_Pipeline',\n",
       " 'identity': {'principal_id': '91200a66-f1b4-4077-860a-4d115e873bc5',\n",
       "  'tenant_id': '72f988bf-86f1-41af-91ab-2d7cd011db47',\n",
       "  'type': 'SystemAssigned'},\n",
       " 'location': 'eastus',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'tags': {},\n",
       " 'sku': 'Basic',\n",
       " 'workspaceid': 'd869dd80-1333-484d-99ae-5869dc1b041a',\n",
       " 'sdkTelemetryAppInsightsKey': '71fa86c2-fe23-49db-bb2e-8537edfb6ead',\n",
       " 'description': '',\n",
       " 'friendlyName': 'Computer_Vision_Pipeline',\n",
       " 'creationTime': '2021-05-17T20:54:23.0413693+00:00',\n",
       " 'containerRegistry': '/subscriptions/83da1f6b-22c0-4300-aa13-bd260eab57e5/resourceGroups/amldatalabeling/providers/Microsoft.ContainerRegistry/registries/d869dd801333484d99ae5869dc1b041a',\n",
       " 'keyVault': '/subscriptions/83da1f6b-22c0-4300-aa13-bd260eab57e5/resourcegroups/amldatalabeling/providers/microsoft.keyvault/vaults/computervision9754617571',\n",
       " 'applicationInsights': '/subscriptions/83da1f6b-22c0-4300-aa13-bd260eab57e5/resourcegroups/amldatalabeling/providers/microsoft.insights/components/computervision0692631043',\n",
       " 'storageAccount': '/subscriptions/83da1f6b-22c0-4300-aa13-bd260eab57e5/resourcegroups/amldatalabeling/providers/microsoft.storage/storageaccounts/computervision5249774111',\n",
       " 'hbiWorkspace': False,\n",
       " 'allowPublicAccessWhenBehindVnet': False,\n",
       " 'provisioningState': 'Succeeded',\n",
       " 'imageBuildCompute': '',\n",
       " 'discoveryUrl': 'https://eastus.api.azureml.ms/discovery',\n",
       " 'notebookInfo': {'fqdn': 'ml-computervisionpi-eastus-d869dd80-1333-484d-99ae-5869dc1b041a.notebooks.azure.net',\n",
       "  'resource_id': '8e585ea2a73c4154a284b9d7ee472237'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from azureml.core.workspace import Workspace, Datastore\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore,Dataset\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws.get_details()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute target setup\n",
    "You will need to provide a [Compute Target](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#computes) that will be used for your AutoML model training. AutoML models for image tasks require [GPU SKUs](https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) such as the ones from the NC, NCv2, NCv3, ND, NDv2 and NCasT4 series. We recommend using the NCsv3-series (with v100 GPUs) for faster training. Using a compute target with a multi-GPU VM SKU will leverage the multiple GPUs to speed up training. Additionally, setting up a compute target with multiple nodes will allow for faster model training by leveraging parallelism, when tuning hyperparameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "cluster_name = \"gpu-cluster-nc6\"\n",
    "\n",
    "try:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    print(\"Found existing compute target.\")\n",
    "except KeyError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_NC6\",\n",
    "        idle_seconds_before_scaledown=600,\n",
    "        min_nodes=0,\n",
    "        max_nodes=4,\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "compute_target.wait_for_completion(\n",
    "    show_output=True, min_node_count=None, timeout_in_minutes=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "Create an [Experiment](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#experiments) in your workspace to track your model training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = \"automl-image-object-detection_latest_V2\"\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with input Training Data\n",
    "\n",
    "Using the AML Labeling tool you labeled your images and exported the annotations as an AML ML Dataset. Now we need to a training_dataset to associate with that export using its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset.get_by_name(ws, name='SodaDemo_20220126_131830')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation dataset is optional. If no validation dataset is specified, by default 20% of your training data will be used for validation. You can control the percentage using the `split_ratio` argument - please refer to the [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models#model-agnostic-hyperparameters) for more details.\n",
    "\n",
    "This is what the training dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>label</th>\n",
       "      <th>label_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computervisionwimagesraw/fa45f236-b599-11eb-8f...</td>\n",
       "      <td>[{'label': 'sprite', 'topX': 0.320216286945812...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>computervisionwimagesraw/895849dc-b4dd-11eb-a2...</td>\n",
       "      <td>[{'label': 'sprite', 'topX': 0.185364070197044...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>computervisionwimagesraw/45.jpg</td>\n",
       "      <td>[{'label': 'sprite', 'topX': 0.078679938956714...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>computervisionwimagesraw/b34aa738-b598-11eb-8f...</td>\n",
       "      <td>[{'label': 'coke', 'topX': 0.3115955972906404,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computervisionwimagesraw/74.jpg</td>\n",
       "      <td>[{'label': 'sprite', 'topX': 0.221694119458128...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>computervisionwimagesraw/a19fb52e-b4f2-11eb-9c...</td>\n",
       "      <td>[{'label': 'diet_coke', 'topX': 0.209645532741...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>computervisionwimagesraw/e45261d0-b599-11eb-8f...</td>\n",
       "      <td>[{'label': 'sprite', 'topX': 0.228467518472906...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>computervisionwimagesraw/80.jpg</td>\n",
       "      <td>[{'label': 'sprite', 'topX': 0.220744311875693...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>computervisionwimagesraw/7170d386-b599-11eb-8f...</td>\n",
       "      <td>[{'label': 'sprite', 'topX': 0.293738454433497...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>computervisionwimagesraw/53.jpg</td>\n",
       "      <td>[{'label': 'diet_coke', 'topX': 0.453836976600...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_url  \\\n",
       "0    computervisionwimagesraw/fa45f236-b599-11eb-8f...   \n",
       "1    computervisionwimagesraw/895849dc-b4dd-11eb-a2...   \n",
       "2                      computervisionwimagesraw/45.jpg   \n",
       "3    computervisionwimagesraw/b34aa738-b598-11eb-8f...   \n",
       "4                      computervisionwimagesraw/74.jpg   \n",
       "..                                                 ...   \n",
       "236  computervisionwimagesraw/a19fb52e-b4f2-11eb-9c...   \n",
       "237  computervisionwimagesraw/e45261d0-b599-11eb-8f...   \n",
       "238                    computervisionwimagesraw/80.jpg   \n",
       "239  computervisionwimagesraw/7170d386-b599-11eb-8f...   \n",
       "240                    computervisionwimagesraw/53.jpg   \n",
       "\n",
       "                                                 label  \\\n",
       "0    [{'label': 'sprite', 'topX': 0.320216286945812...   \n",
       "1    [{'label': 'sprite', 'topX': 0.185364070197044...   \n",
       "2    [{'label': 'sprite', 'topX': 0.078679938956714...   \n",
       "3    [{'label': 'coke', 'topX': 0.3115955972906404,...   \n",
       "4    [{'label': 'sprite', 'topX': 0.221694119458128...   \n",
       "..                                                 ...   \n",
       "236  [{'label': 'diet_coke', 'topX': 0.209645532741...   \n",
       "237  [{'label': 'sprite', 'topX': 0.228467518472906...   \n",
       "238  [{'label': 'sprite', 'topX': 0.220744311875693...   \n",
       "239  [{'label': 'sprite', 'topX': 0.293738454433497...   \n",
       "240  [{'label': 'diet_coke', 'topX': 0.453836976600...   \n",
       "\n",
       "                                  label_confidence  \n",
       "0                   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "1                   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "2                   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "3                   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "4                   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "..                                             ...  \n",
       "236                 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "237                 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "238                 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "239                 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "240  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "\n",
       "[241 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring your AutoML run for image tasks\n",
    "AutoML allows you to easily train models for Image Classification, Object Detection & Instance Segmentation on your image data. You can control the model algorithm to be used, specify hyperparameter values for your model as well as perform a sweep across the hyperparameter space to generate an optimal model. Parameters for configuring your AutoML Image run are specified using the `AutoMLImageConfig` - please refer to the [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models#configure-your-experiment-settings) for the details on the parameters that can be used and their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using AutoML for image tasks, you need to specify the model algorithms using the `model_name` parameter. You can either specify a single model or choose to sweep over multiple models. Please refer to the [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models#configure-model-algorithms-and-hyperparameters) for the list of supported model algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using default hyperparameter values for the specified algorithm\n",
    "Before doing a large sweep to search for the optimal models and hyperparameters, we recommend trying the default values for a given model to get a first baseline. Next, you can explore multiple hyperparameters for the same model before sweeping over multiple models and their parameters. This allows an iterative approach, as with multiple models and multiple hyperparameters for each (as we showcase in the next section), the search space grows exponentially, and  you need more iterations to find optimal configurations.\n",
    "\n",
    "If you wish to use the default hyperparameter values for a given algorithm (say `yolov5`), you can specify the config for your AutoML Image runs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared.constants import ImageTask\n",
    "from azureml.train.automl import AutoMLImageConfig\n",
    "from azureml.train.hyperdrive import GridParameterSampling, choice\n",
    "\n",
    "image_config_yolov5 = AutoMLImageConfig(\n",
    "    task=ImageTask.IMAGE_OBJECT_DETECTION,\n",
    "    compute_target=compute_target,\n",
    "    training_data=training_dataset,\n",
    "    #validation_data=validation_dataset,\n",
    "    hyperparameter_sampling=GridParameterSampling({\"model_name\": choice(\"yolov5\")}),\n",
    "    iterations=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting an AutoML run for Computer Vision tasks\n",
    "Once you've created the config settings for your run, you can submit an AutoML run using the config in order to train a vision model using your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting remote run.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>automl-image-object-detection_latest_V2</td><td>AutoML_2e890e19-a522-4102-9ed7-371db3c78d1d</td><td>automl</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/AutoML_2e890e19-a522-4102-9ed7-371db3c78d1d?wsid=/subscriptions/83da1f6b-22c0-4300-aa13-bd260eab57e5/resourcegroups/amldatalabeling/workspaces/Computer_Vision_Pipeline&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "automl_image_run = experiment.submit(image_config_yolov5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_image_run.wait_for_completion(wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing a hyperparameter sweep, it can be useful to visualize the different configurations that were tried using the HyperDrive UI. You can navigate to this UI by going to the 'Child runs' tab in the UI of the main `automl_image_run` from above, which is the HyperDrive parent run. Then you can go into the 'Child runs' tab of this HyperDrive parent run. Alternatively, here below you can see directly the HyperDrive parent run and navigate to its 'Child runs' tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the optimal vision model from the AutoML run\n",
    "Once the run completes, we can register the model that was created from the best run (configuration that resulted in the best primary metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model from the best run\n",
    "\n",
    "best_child_run = automl_image_run.get_best_child()\n",
    "model_name = best_child_run.properties[\"model_name\"]\n",
    "model = best_child_run.register_model(\n",
    "    model_name=model_name, model_path=\"outputs/model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model and other associated files e.g. labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/how-to-inference-onnx-automl-image-models.md\n",
    "# Create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "# Download the model from run history\n",
    "best_child_run.download_file(name='train_artifacts/model.onnx',\n",
    "output_file_path='./model/model.onnx')\n",
    "\n",
    "best_child_run.download_file(name='train_artifacts/labels.json',\n",
    "output_file_path='./model/labels.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ***Optional: if you trained the model earlier you can still down load the model and label  using Run id. This will be the highest level run id from a child run perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best child run\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "import json\n",
    "\n",
    "run_id = \"AutoML_a756bc5d-d85a-4178-9c36-0877cdc6dfbf\" # Specify the run ID\n",
    "automl_image_run = AutoMLRun(experiment=experiment, run_id=run_id)\n",
    "best_child_run = automl_image_run.get_best_child()\n",
    "\n",
    "labels_file = \"./model/labels.json\"\n",
    "best_child_run.download_file(name=\"train_artifacts/labels.json\", output_file_path=labels_file)\n",
    "\n",
    "onnx_model_path = \"./model/model.onnx\"\n",
    "best_child_run.download_file(name=\"train_artifacts/model.onnx\", output_file_path=onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the labels and ONNX model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import onnxruntime\n",
    "#import json\n",
    "\n",
    "#labels_file = \"./model/labels.json\"\n",
    "#onnx_model_path = \"./model/model.onnx\"\n",
    "\n",
    "\n",
    "#with open(labels_file) as f:\n",
    "#    classes = json.load(f)\n",
    "#print(classes)\n",
    "#try:\n",
    "#    session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "#    print(\"ONNX model loaded...\")\n",
    "#except Exception as e: \n",
    "#    print(\"Error loading ONNX file: \",str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expected input and output details for an ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess_input = session.get_inputs()\n",
    "#sess_output = session.get_outputs()\n",
    "#print(f\"No. of inputs : {len(sess_input)}, No. of outputs : {len(sess_output)}\")\n",
    "\n",
    "#for idx, input_ in enumerate(range(len(sess_input))):\n",
    "#    input_name = sess_input[input_].name\n",
    "#    input_shape = sess_input[input_].shape\n",
    "#    input_type = sess_input[input_].type\n",
    "#    print(f\"{idx} Input name : { input_name }, Input shape : {input_shape}, \\\n",
    "#    Input type  : {input_type}\")  \n",
    "\n",
    "#for idx, output in enumerate(range(len(sess_output))):\n",
    "#    output_name = sess_output[output].name\n",
    "#    output_shape = sess_output[output].shape\n",
    "#    output_type = sess_output[output].type\n",
    "#    print(f\" {idx} Output name : {output_name}, Output shape : {output_shape}, \\\n",
    "#    Output type  : {output_type}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Inferencing Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the helper module yolo_onnx_preprocessing_utils to the system path where python looks for modules\n",
    "#import sys\n",
    "#MODULE_FULL_PATH = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/rdurham1/code/Users/rdurham/GTC/'\n",
    "#sys.path.insert(1, MODULE_FULL_PATH)\n",
    "#import yolo_onnx_preprocessing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from yolo_onnx_preprocessing_utils import preprocess\n",
    "\n",
    "#test_image_path = \"automl_models_od_yolo/test_image1.jpg\"\n",
    "#img_data, pad = preprocess(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_predictions_from_ONNX(onnx_session,img_data):\n",
    "#    \"\"\"perform predictions with ONNX Runtime\n",
    "#    \n",
    "#    :param onnx_session: onnx model session\n",
    "#    :type onnx_session: class InferenceSession\n",
    "#    :param img_data: pre-processed numpy image\n",
    "#    :type img_data: ndarray with shape 1xCxHxW\n",
    "#    :return: boxes, labels , scores \n",
    "#    :rtype: list\n",
    "#    \"\"\"\n",
    "#    sess_input = onnx_session.get_inputs()\n",
    "#    sess_output = onnx_session.get_outputs()\n",
    "#    # predict with ONNX Runtime\n",
    "#    output_names = [ output.name for output in sess_output]\n",
    "#    pred = onnx_session.run(output_names=output_names,\\\n",
    "#                                               input_feed={sess_input[0].name: img_data})\n",
    "#    return pred[0]\n",
    "#\n",
    "#result = get_predictions_from_ONNX(session, img_data)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch, channel, height_onnx, width_onnx = session.get_inputs()[0].shape\n",
    "#batch, channel, height_onnx, width_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from yolo_onnx_preprocessing_utils import non_max_suppression, _convert_to_rcnn_output\n",
    "\n",
    "#result = non_max_suppression(\n",
    "#    torch.from_numpy(np.asarray(result)),\n",
    "#    conf_thres=.1,\n",
    "#   iou_thres=.5)\n",
    "\n",
    "\n",
    "#label, image_shape = _convert_to_rcnn_output(result[0], height_onnx, width_onnx, pad)\n",
    "#boxes = np.array(label[\"boxes\"])\n",
    "#labels = np.array(label[\"labels\"])\n",
    "#labels = [label[0] for label in labels]\n",
    "#scores = np.array(label[\"scores\"])\n",
    "#scores = [score[0] for score in scores]\n",
    "#boxes, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def _get_box_dims(image_shape, box):\n",
    "#    box_keys = ['topX', 'topY', 'bottomX', 'bottomY']\n",
    "#    height, width = image_shape[0], image_shape[1]\n",
    "\n",
    "#    box_dims = dict(zip(box_keys, [coordinate.item() for coordinate in box]))\n",
    "\n",
    "#    box_dims['topX'] = box_dims['topX'] * 1.0 / width\n",
    "#    box_dims['bottomX'] = box_dims['bottomX'] * 1.0 / width\n",
    "#    box_dims['topY'] = box_dims['topY'] * 1.0 / height\n",
    "#    box_dims['bottomY'] = box_dims['bottomY'] * 1.0 / height\n",
    "\n",
    "#    return box_dims\n",
    "\n",
    "#def _get_prediction(boxes, labels, scores, image_shape, classes):\n",
    "#    bounding_boxes = []\n",
    "#    for box, label_index, score in zip(boxes, labels, scores):\n",
    "#        box_dims = _get_box_dims(image_shape, box)\n",
    "\n",
    "#        box_record = {'box': box_dims,\n",
    "#                      'label': classes[label_index],\n",
    "#                      'score': score.item()}\n",
    "\n",
    "#        bounding_boxes.append(box_record)\n",
    "\n",
    "#    return bounding_boxes\n",
    "\n",
    "#bounding_boxes = _get_prediction(boxes, labels, scores, (height_onnx,width_onnx), classes)\n",
    "#print(json.dumps(bounding_boxes, indent=1))\n",
    "\n",
    "# Filter the results with a threshold.\n",
    "# Replace the threshold for your test scenario.\n",
    "#score_threshold = 0.6\n",
    "#filtered_bounding_boxes = []\n",
    "#for box in bounding_boxes:\n",
    "#    if box['score'] >= score_threshold:\n",
    "#        filtered_bounding_boxes.append(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "#import matplotlib.patches as patches\n",
    "#from PIL import Image\n",
    "#%matplotlib inline\n",
    "\n",
    "#IMAGE_SIZE = (18,12)\n",
    "#plt.figure(figsize=IMAGE_SIZE)\n",
    "#img_np = mpimg.imread(test_image_path)\n",
    "#img = Image.fromarray(img_np.astype('uint8'),'RGB')\n",
    "#x, y = img.size\n",
    "#print(img.size)\n",
    "\n",
    "#fig,ax = plt.subplots(1)\n",
    "# Display the image\n",
    "#ax.imshow(img_np)\n",
    "\n",
    "# Draw a box and label for each detection \n",
    "#for detect in filtered_bounding_boxes:\n",
    "#    label = detect['label']\n",
    "#    box = detect['box']\n",
    "#    ymin, xmin, ymax, xmax =  box['topY'],box['topX'], box['bottomY'],box['bottomX']\n",
    "#    topleft_x, topleft_y = x  * 1* xmin, y * 1.45 * ymin\n",
    "#    width, height = x * (xmax - xmin), y * (ymax - ymin)\n",
    "#    print('{}: {}, {}, {}, {}'.format(detect['label'], topleft_x, topleft_y, width, height))\n",
    "#    rect = patches.Rectangle((topleft_x, topleft_y), width, height, \n",
    "#                             linewidth=1, edgecolor='green',facecolor='none')\n",
    "\n",
    "#    ax.add_patch(rect)\n",
    "#    color = 'green'\n",
    "#    plt.text(topleft_x, topleft_y, label, color=color)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
